{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup APIs and Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "with open('setup.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "for key, value in config['environment'].items():\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatGroq(model='llama3-70b-8192')\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "llm_response = llm.invoke(\"Tell me a joke!\")\n",
    "\n",
    "llm_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PostgreSQL-DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"postgresql://postgres:postgres@localhost:5432/postgres\", include_tables=[\"cryptocurrencies\", \"market_data\"])\n",
    "\n",
    "print(db.get_usable_table_names())\n",
    "db.run(\"SELECT * FROM market_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the tables schema along with example table data\n",
    "scehma = db.get_table_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the YAML file\n",
    "with open(\"prompts/prompts.yml\", \"r\") as file:\n",
    "    prompts = yaml.safe_load(file)\n",
    "\n",
    "# Access the prompts\n",
    "analyst_sys_prompt = prompts[\"system_prompts\"][\"analyst_llm\"][\"description\"]\n",
    "supervisor_llm_prompt = prompts[\"system_prompts\"][\"supervisor_llm\"][\"description\"]\n",
    "advisor_llm_prompt = prompts[\"system_prompts\"][\"advisor_llm\"][\"description\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    sql_query: str\n",
    "    query_result: str\n",
    "    sources: list[str]\n",
    "    web_results: list[str]\n",
    "    summarized_results: list[str]\n",
    "    viz_code: str\n",
    "    agents: str\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advisor Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "# from google.colab import files\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Initialize the FAISS vector store\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# File paths\n",
    "index_file = \"index.faiss\"\n",
    "chunks_file = \"chunks.txt\"\n",
    "\n",
    "# Cryptocurrency-related keywords (need to add more terms if possible)\n",
    "CRYPTO_KEYWORDS = {\"crypto\", \"cryptocurrency\", \"bitcoin\", \"ethereum\", \"blockchain\", \"web3\",\n",
    "                   \"decentralized\", \"mining\", \"token\", \"NFT\", \"stablecoin\", \"defi\", \"ledger\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_text(pdf_path):\n",
    "    \"\"\"Extract and store PDF text if it matches the cryptocurrency topic.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "\n",
    "    # Check for cryptocurrency-related keywords\n",
    "    if not any(keyword in text.lower() for keyword in CRYPTO_KEYWORDS):\n",
    "        raise ValueError(\"PDF content does not match the required topic: Cryptocurrency\")\n",
    "\n",
    "    # Split text into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = embedding_model.encode(chunks)\n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Save the FAISS index\n",
    "    faiss.write_index(index, index_file)\n",
    "\n",
    "    # Save the text chunks\n",
    "    with open(chunks_file, \"w\") as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(chunk + \"\\n\")\n",
    "\n",
    "    print(\"Cryptocurrency-related PDF processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    extract_and_store_text('pdfs/Mastering-Crypto-Day-Trading-From-Blockchain-Basics-to-Profits.pdf')\n",
    "except ValueError as ve:\n",
    "    print(f\"Error: {ve}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, top_k=3):\n",
    "    \"\"\"Retrieve the top-k most relevant text chunks from FAISS.\"\"\"\n",
    "    if not os.path.exists(index_file):\n",
    "        raise FileNotFoundError(\"FAISS index not found. Please upload a valid PDF first.\")\n",
    "\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_file)\n",
    "\n",
    "    # Generate the query embedding\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    query_embedding = np.array(query_embedding, dtype=np.float32)\n",
    "\n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # Retrieve the corresponding text chunks\n",
    "    with open(chunks_file, \"r\") as f:\n",
    "        text_chunks = f.readlines()\n",
    "\n",
    "    retrieved_chunks = [text_chunks[i].strip() for i in indices[0] if i < len(text_chunks)]\n",
    "\n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advisor_agent(state: State):\n",
    "    \"\"\"Generate a response from the advisor agent using FAISS-based context and Groq API.\"\"\"\n",
    "    try:\n",
    "        # Retrieve context\n",
    "        retrieved_chunks = retrieve_relevant_chunks(state[\"question\"])\n",
    "        context = \"\\n\".join(retrieved_chunks) if retrieved_chunks else \"No relevant context found.\"\n",
    "\n",
    "        # Construct the chat prompt with the strict cryptocurrency-only system prompt\n",
    "        advisor_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", advisor_llm_prompt),\n",
    "            (\"human\", f\"Context: {context}\\n\\nUser Query: {state[\"question\"]}\")\n",
    "        ])\n",
    "\n",
    "        # Initialize Groq LLM with the correct model\n",
    "        groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        llm = ChatGroq(model=\"mixtral-8x7b-32768\", api_key=groq_api_key)\n",
    "\n",
    "        # Generate response using LangChain LLM\n",
    "        response = advisor_prompt | llm\n",
    "        result = response.invoke({})\n",
    "\n",
    "        # Ensure the response is not mixed and only provides relevant output\n",
    "        response_text = result.content.strip()\n",
    "\n",
    "        state[\"response\"] = response_text\n",
    "\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_question = {\"question\": \"What is the best way to invest in cryptocurrencies?\"}\n",
    "# response = advisor_agent(user_question)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Educate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from re import search\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing_extensions import TypedDict\n",
    "import re\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "summary_template = \"\"\"\n",
    "Summarize the following content into a concise paragraph that directly addresses the query. Ensure the summary \n",
    "highlights the key points relevant to the query while maintaining clarity and completeness.\n",
    "Query: {query}\n",
    "Content: {content}\n",
    "\"\"\"\n",
    "\n",
    "class CheckPrompt(BaseModel):\n",
    "    prompt_type: str = Field(description=\"Category of the prompt: YES or NO\")\n",
    "\n",
    "def search_web(state: State):\n",
    "    search = TavilySearchResults(max_results=3)\n",
    "    search_results = search.invoke(state[\"question\"])\n",
    "\n",
    "    state[\"sources\"] = [result['url'] for result in search_results]\n",
    "    state[\"web_results\"] = [result['content'] for result in search_results]\n",
    "\n",
    "    return state\n",
    "\n",
    "def summarize_results(state: State):\n",
    "    prompt = ChatPromptTemplate.from_template(summary_template)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    summarized_results = []\n",
    "    for content in state[\"web_results\"]:\n",
    "        summary = chain.invoke({\"query\": state[\"question\"], \"content\": content})\n",
    "        clean_content = clean_text(summary.content)\n",
    "        summarized_results.append(clean_content)\n",
    "    \n",
    "    state[\"summarized_results\"] = summarized_results\n",
    "\n",
    "    return state\n",
    "\n",
    "def clean_text(text: str):\n",
    "    cleaned_text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyst LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_sql_query_chain\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "analyst_prompt = prompt = PromptTemplate(\n",
    "    input_variables=[\"dialect\", \"table_info\", \"input\", \"top_k\"],\n",
    "    template=analyst_sys_prompt\n",
    ")\n",
    "\n",
    "def analyst_agent(state: State):\n",
    "    question = state['question']\n",
    "    generate_query = create_sql_query_chain(llm, db, prompt=analyst_prompt)\n",
    "    try:\n",
    "        query = generate_query.invoke({\"question\": question}) \n",
    "        state['sql_query']=query.split('SQLQuery: ')[1]\n",
    "    except:\n",
    "        state['sql_query'] = \"\"\n",
    "        \n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "class sqlquery(BaseModel):\n",
    "    sql_query: str = Field(..., title=\"Syntactically correct SQL Query\")\n",
    "\n",
    "def correct_query(state: State): \n",
    "    query = state[\"sql_query\"]\n",
    "    question = state['question']\n",
    "    schema = db.get_table_info()\n",
    "    \n",
    "    query_check_system = f\"\"\"You are a Postgres SQL expert with a strong attention to detail.\n",
    "    For the given question: {question}, double check the Postgres SQL query if it is relevant to the given question and check for common mistakes, including:\n",
    "    - Using NOT IN with NULL values\n",
    "    - Using UNION when UNION ALL should have been used\n",
    "    - Using BETWEEN for exclusive ranges\n",
    "    - Data type mismatch in predicates\n",
    "    - Properly quoting identifiers\n",
    "    - Using the correct number of arguments for functions\n",
    "    - Casting to the correct data type\n",
    "    - Using the proper columns for joins\n",
    "\n",
    "    If there are any of the above mistakes or the query is irrelevant to the given question, rewrite the query. \n",
    "    If there are no mistakes, just reproduce the original query.\n",
    "    DO NOT provide any explanations, comments, or additional context.\n",
    "    ONLY output the SQL query itself.\n",
    "\n",
    "    Here's the original table schema:\n",
    "    {schema}\n",
    "\n",
    "    \"\"\"\n",
    "    max_retries = 3\n",
    "    retry_delay = 2\n",
    "\n",
    "    query_check_prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", query_check_system), \n",
    "         (\"human\", f\"{query}\")]\n",
    "    )\n",
    "\n",
    "    query_check = query_check_prompt | llm.with_structured_output(sqlquery) \n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            query = query_check.invoke({}).sql_query\n",
    "            response = db.run_no_throw(query)\n",
    "            \n",
    "            if 'Error' in response:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(retry_delay * (attempt + 1))  # Exponential backoff\n",
    "                    continue\n",
    "                else:\n",
    "                    state['sql_query'] = \"\"\n",
    "                    break\n",
    "            \n",
    "            # Extract the SQL query if successful\n",
    "            state['sql_query'] = query\n",
    "            break\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay * (attempt + 1))\n",
    "                continue\n",
    "            else:\n",
    "                state['sql_query'] = \"\"\n",
    "                break\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_query(state={\"question\":\"What was the price of bitcoin last month vs now with dates?\",\"sql_query\": \"SELECT \\n    m1.date AS 'last_month_date', \\n    m1.prices AS 'last_month_price', \\n    m2.date AS 'current_date', \\n    m2.prices AS 'current_price'\\nFROM \\n    market_data m1\\nJOIN \\n    market_data m2 ON m1.id = m2.id\\nWHERE \\n    m1.id = \\'bitcoin\\' \\n    AND m1.date = (CURRENT_DATE - INTERVAL \\'1 month\\') \\n    AND m2.date = CURRENT_DATE\\nLIMIT 1;\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the query\n",
    "from langchain_community.tools import QuerySQLDataBaseTool\n",
    "\n",
    "def execute_query(state: State):\n",
    "    execute_query_tool = QuerySQLDataBaseTool(db=db)\n",
    "    try: \n",
    "        result = execute_query_tool.invoke(state[\"sql_query\"])\n",
    "        state[\"response\"] = result\n",
    "        state['query_result'] = result\n",
    "    except:\n",
    "        state[\"response\"] = \"\"\n",
    "        state['query_result'] = \"\"\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "def generate_response(state: State):\n",
    "  \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"\"\"\n",
    "                Given the following user query and context, generate a response that directly answers the user query using relevant \n",
    "                information from the context. Ensure that the response is clear, concise, and well-structured. \n",
    "\n",
    "                Question: {question} \n",
    "                Context: {context} \n",
    "                sources: {sources}\n",
    "\n",
    "                Answer:\n",
    "             \"\"\"\n",
    "             )\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt | llm \n",
    "\n",
    "    if 'sources' not in state:\n",
    "\n",
    "        response = chain.invoke({\"question\": state[\"question\"], \"context\": state[\"response\"], \"sources\": ''})\n",
    "        state[\"response\"] = response.content\n",
    "        \n",
    "        return state\n",
    "\n",
    "    content = \"\\n\\n\".join([summary for summary in state[\"summarized_results\"]])\n",
    "    response = chain.invoke({\"question\": state[\"question\"], \"context\": content, \"sources\": state['sources']})\n",
    "    # state['response'] = response.content\n",
    "    state['response'] = response.content + '\\n\\n'+ '#### References' + '\\n' + '\\n'.join(state['sources'])\n",
    "\n",
    "    return state\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisor Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "class QueryOutput(BaseModel):\n",
    "    category: str = Field(description=\"Category of the query: analyst, prof, or finish\")\n",
    "\n",
    "def supervisor_agent(state: State): \n",
    "\n",
    "    question = state[\"question\"]\n",
    "    if not question == 'finish':\n",
    "        # print(f\"Selecting the agent to answer the question: {question}\")\n",
    "        system = supervisor_llm_prompt\n",
    "        human = f\"Question: {question}\"\n",
    "        check_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\"human\", human),\n",
    "            ]\n",
    "        )\n",
    "        structured_llm = llm.with_structured_output(QueryOutput)\n",
    "        agent_finder = check_prompt | structured_llm\n",
    "        response = agent_finder.invoke({})\n",
    "        state[\"agents\"] = response.category\n",
    "        # print(f\"Agent Selected: {state['agents']}\")\n",
    "        return state\n",
    "    \n",
    "    state[\"agents\"] = 'finish'\n",
    "    # print(f\"Agent Selected: {state['agents']}\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_agent(state={\"question\": \"can you advise me on how to invest in cryptocurrencies?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class python_code(BaseModel):\n",
    "    python_code: str = Field(..., title=\"Syntactically correct python code\")\n",
    "\n",
    "def generate_chart(state: State):\n",
    "\n",
    "    if \"show\" in state[\"question\"].lower() or \"plot\" in state[\"question\"].lower() or \"generate\" in state[\"question\"].lower():\n",
    "        question = state[\"question\"]\n",
    "        query_result = state[\"query_result\"]\n",
    "        response = state[\"response\"]\n",
    "        # web_results = state[\"web_results\"]\n",
    "        \n",
    "        if not query_result == \"\": # In case the query was successful\n",
    "\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\",f\"\"\" Given the user question {question}, and the human readable answer {response}, \n",
    "    \n",
    "                    you are a data analytics expert who will write clean python code to generate intuitive meaningful plots to visualize the data on plotly. \n",
    "                    The generated output should only include python code and nothing more. \n",
    "                    \n",
    "                    ** STRICTLY DO NOT MENTION any extra details apart from the python code. **\n",
    "                    ** PROVIDE SYNTACTICALLY CORRECT PYTHON CODE ONLY. ENSURE EVERYTHING IS DEFINED PROPERLY**\n",
    "                    \"\"\"),\n",
    "                ]\n",
    "            )\n",
    "            chain = prompt | llm.with_structured_output(python_code)\n",
    "            \n",
    "            code_response = chain.invoke({})\n",
    "            # Execute the generated Python code directly\n",
    "            state[\"viz_code\"] = code_response.python_code\n",
    "\n",
    "            return state\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Routers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_router(state: State):\n",
    "    if state[\"agents\"].lower() == \"analyst\":\n",
    "        return \"analyst\"\n",
    "    elif state[\"agents\"].lower() == \"prof\":\n",
    "        return \"prof\"\n",
    "    elif state[\"agents\"].lower() == \"advisor\":\n",
    "        return \"advisor\"\n",
    "    else:\n",
    "        return \"Irrelevant Query\"\n",
    "    \n",
    "def end_node(state: State):\n",
    "    \n",
    "    state['response'] = \"Sorry, I can only answer questions relating to the digital assets.\"\n",
    "\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,START,END\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"analyst\", analyst_agent)\n",
    "workflow.add_node(\"advisor\", advisor_agent)\n",
    "workflow.add_node(\"correct_query\", correct_query)\n",
    "workflow.add_node(\"execute_sql\", execute_query)\n",
    "workflow.add_node(\"prof\", search_web)\n",
    "workflow.add_node(\"summarize_results\", summarize_results)\n",
    "workflow.add_node(\"generate_chart\", generate_chart)\n",
    "workflow.add_node(\"end_node\", end_node)\n",
    "workflow.add_node(\"generate_response\", generate_response)\n",
    "# workflow.add_node(\"redirect_node\", redirect_node)\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    supervisor_router,\n",
    "    {\n",
    "        \"analyst\": \"analyst\",\n",
    "        \"prof\": \"prof\",\n",
    "        \"advisor\": \"advisor\",\n",
    "        \"Irrelevant Query\": \"end_node\"\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"prof\", \"summarize_results\")\n",
    "workflow.add_edge(\"summarize_results\", \"generate_response\")\n",
    "\n",
    "workflow.add_edge(\"analyst\", \"correct_query\")\n",
    "workflow.add_edge(\"correct_query\", \"execute_sql\")\n",
    "# workflow.add_edge(\"execute_sql\", \"redirect_node\")\n",
    "workflow.add_edge(\"execute_sql\", \"generate_response\")\n",
    "\n",
    "workflow.add_edge(\"advisor\", \"generate_response\")\n",
    "workflow.add_edge(\"generate_response\", \"generate_chart\")\n",
    "workflow.add_edge(\"generate_chart\", END)\n",
    "workflow.add_edge(\"end_node\", END)\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"question\": \"can you advise me on how to invest in cryptocurrencies?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    user_input = input(\"\")\n",
    "\n",
    "    if user_input == \"q\":\n",
    "        break\n",
    "    \n",
    "    print(\"You:\", user_input)\n",
    "    for event in app.stream({'question': f\"{user_input}\"}):\n",
    "\n",
    "        for value in event.values():\n",
    "            response = value.get('response')\n",
    "    \n",
    "    if response is not None: \n",
    "        print('Bot:', response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = app.invoke({\"question\": \"How have the various crypto sectors performed over the lastÂ few months?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
